\documentclass[10pt]{article}
\input{../doc/report/header}
\input{../doc/report/defs}

\begin{document}

\section{Proposal scratch notes}

\begin{itemize}

\item
$\calD$, $\calI$: (unknown) disjoint sets of indices that describe the groups of transformations that are relevant and irrelevant to the output, respectively. $\calD\cup\calI=\{1,\ldots,m\}$.

\item
$U_Y$, $U_\calI$, $U_\calD$, $\tildeU_\calI$: independent latent variables that influence the value of the variable(s) that they point to.

\item
$X^{\text{(hid)}}$: some unknown canonical form of the observed input $X$.  It is assumed that given $U_\calD$ and $U_\calI$, $X$ was obtained from an ordered sequence of transformations on the canonical form, i.e.,
\[
X=T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}
\]
where transformations
\[
T_{U_\calD,U_\calI}=T_\calI^{(1)}\circ T_\calD^{(1)}\circ T_\calI^{(2)}\circ\ldots
\]
make up the overgroup $\calG_{\calD\cup\calI}$, $T_\calD^{(j)}$ is a transformation in group $\calG_j$ from the overgroup $\calG_\calD=\langle\cup_{j\in\calD}\calG_j\rangle$, and $T_\calI^{(i)}\in\calG_i\subset\calG_\calI=\langle\cup_{i\in\calI}\calG_i\rangle$. Note that $\calG_\calI$ is also assumed to be a normal subgroup of $\calG_{\calD\cup\calI}$.

\item
$Y$: observed output assumed to be generated by
\[
Y = h(X^{\text{(hid)}},U_\calD,U_Y)
\]
where $h$ is a deterministic function.

\item
$X_{U_\calI\leftarrow\tildeU_\calI}$: counterfactual variable to $X$ where $U_\calI$ has been replaced by $\tildeU_\calI$, i.e.,
\[
X_{U_\calI\leftarrow\tildeU_\calI} = T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}} \;.
\]

\item
Want CG-invariant representation
\[
\Gamma(X) = \Gamma(T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}) = \Gamma(T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}}) =  \Gamma(X_{U_\calI\leftarrow\tildeU_\calI})
\]
When $\calG_\calI$ is a normal subgroup of $\calG_{\calD\cup\calI}$, G-invariant representation
\[
\Gamma(X) = \Gamma(T_\calI\circ X)
\]
for all $T_\calI\in\calG_\calI$ is sufficient.

\item
Because groups are finite linear automorphisms, each transformation $T$ is just a linear function and so Reynolds operator can be applied directly to the group actions
\[
\bar{T} = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
For continuous linear groups, orbit-average over a Haar measure $\lambda$
\[
\bar{T} = \int_\calG T\lambda(T)
\]
\todo: need to estimate the operator. Assume uniform Haar and sample?

\end{itemize}

\section{Kernel hypothesis test notes}

\begin{itemize}

\item
Let $X$ be a r.v. on domain $\calX$. A kernel $k:\calX\times\calX\rightarrow\mathbb{R}$ induces a RHKS $\mathcal{H}$ of functions $f:\calX\rightarrow\mathbb{R}$ where for $f\in\mathcal{H}$ and $x\in\calX$,
\[
\langle f,k(x,\cdot)\rangle = f(x)
\]
(reproducing property). $k(x,\cdot)=\phi_x$ can also be considered an implicit feature map ($\phi_x:\calX\rightarrow\mathbb{R}$) where
\[
\langle \phi_x,\phi_{x'}\rangle = k(x,x')
\]
is a measure of similarity.

\item
Kernel methods work on inner products of feature maps of observations in the RKHS associated with kernel. Inner products may be computed without explicitly computing the high-dimensional feature map (``kernel trick'').

\item
Main challenge of designing kernel-based hypothesis tests is deriving large-sample distribution of test statistic under null.

\item
Gram matrix should be positive semidefinite. Satisfised if kernel is symmetric and positive semidefinite.

\item
Let $X$ be a r.v. with distribution $\mathbb{P}$. \href{https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions}{Mean element}
\[
\mu_\mathbb{P} = \mathbb{E}_{\mathbb{P}}[\phi_X]
\]
associated with $X$ is unique element of RKHS $\mathcal{H}$ s.t. for all $f\in\mathcal{H}$,
\[
\langle\mu_\mathbb{P},f\rangle = \mathbb{E}_\mathbb{P}[f(X)]
\]
Covariance operator $\Sigma_\mathbb{P}:\mathcal{H}\times \mathcal{H}\rightarrow\mathbb{R}$ associated with $X$ is unique operator s.t. for all $f,g\in\mathcal{H}$,
\[
\langle f, \Sigma_\mathbb{P}g\rangle = \mathrm{Cov}(f(X),g(X)) = \mathbb{E}_\mathbb{P}[f(X),g(X)] - \langle\mu_\mathbb{P},f\rangle\langle\mu_\mathbb{P},g\rangle
\]
Empirical estimates of inner products that lead to estimates of element/operator are available.

\item
Kernel is characteristic if mean embedding $\mu:\mathbb{P}\rightarrow\mathcal{H}$ is injective. Each distribution can be uniquely represented in the RKHS and all statistica features of distributions are preserved (\todo) by a characteristic kernel.

\item
If $\mathrm{dim}(\mathcal{H})=\infty$, $\mu_\mathbb{P}$ has more significance than in classical statistics.

\item
(Kellner,2015) MMD(? or related quantity) is a pseudo-metric. If restricting space of functions (e.g., r.v. space) to unit ball of RKHS with positive semi-definite characteristic kernel, MMD is a metric.

\end{itemize}

\section{Detecting conditional invariances in single training environment via hypothesis testing}

\begin{itemize}

\item
Case of invariance of output to single known group acting on inputs. Following (Mouli, 2021) in that we specify potential group of invariant transformations and testing if dataset contradicts invariance to the group, rather than if dataset appears to be invariant to group.

\item
Other assumptions?

\item
Possible two-sample test procedure: generate transformed inputs as ``second'' sample and compare conditional mean embeddings (e.g., Gretton, 2007). How should embeddings be estimated? (Song, 2013) Conditional embedding operator is a family of points in RKHS. Only when conditioned on a fixed value is it a single point.
\\

Given dataset $\{(x_i,y_i)\}_{i=1}^N$ assumed to be from some joint distribution $\mathbb{P}_{XY}=\mathbb{P}_X\mathbb{P}_{Y|X}$, want to determine if $\mathbb{P}_{Y|X}\overset{d}{=}\mathbb{P}_{Y|\calG\cdot X}$ for group $\calG$ where $\mathbb{P}_{Y|\calG\cdot X}(y|x)=\mathbb{P}_{Y|\calG\cdot X}(y|g\cdot x)$ for all $g\in\calG$, $x\in\calX$. (Elesedy, 2021) obtains invariant functions by orbit-averaging. We want a metric of some form \todo
\[
\mathrm{MMD}(\mathbb{P},\mathbb{P}') = \|\mu_{Y|X} - \mu_{Y|\calG\cdot X}\|_\mathcal{H}^2
\]
How to handle $X$ and $\mu_{Y|X}$ being a family of functions? $\mu_{Y|\calG\cdot X}$ estimated by generating/summing/integration transformed inputs?
\\

Comparing joint distributions only makes sense if it is assumed $\mathbb{P}(X)=\mathbb{P}(\calG\cdot X)$? This does not make sense for extrapolation context ($Y|X$ does not have to be invariant then?).

\item
One-sample test? Via parametric bootstrap (Kellner, 2015)?

\end{itemize}

\end{document}