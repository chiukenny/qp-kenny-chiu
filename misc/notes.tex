\documentclass[10pt]{article}
\input{../doc/report/header}
\input{../doc/report/defs}

\begin{document}

\section{Proposal scratch notes}

\begin{itemize}

\item
$\calD$, $\calI$: (unknown) disjoint sets of indices that describe the groups of transformations that are relevant and irrelevant to the output, respectively. $\calD\cup\calI=\{1,\ldots,m\}$.

\item
$U_Y$, $U_\calI$, $U_\calD$, $\tildeU_\calI$: independent latent variables that influence the value of the variable(s) that they point to.

\item
$X^{\text{(hid)}}$: some unknown canonical form of the observed input $X$.  It is assumed that given $U_\calD$ and $U_\calI$, $X$ was obtained from an ordered sequence of transformations on the canonical form, i.e.,
\[
X=T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}
\]
where transformations
\[
T_{U_\calD,U_\calI}=T_\calI^{(1)}\circ T_\calD^{(1)}\circ T_\calI^{(2)}\circ\ldots
\]
make up the overgroup $\calG_{\calD\cup\calI}$, $T_\calD^{(j)}$ is a transformation in group $\calG_j$ from the overgroup $\calG_\calD=\langle\cup_{j\in\calD}\calG_j\rangle$, and $T_\calI^{(i)}\in\calG_i\subset\calG_\calI=\langle\cup_{i\in\calI}\calG_i\rangle$. Note that $\calG_\calI$ is also assumed to be a normal subgroup of $\calG_{\calD\cup\calI}$.

\item
$Y$: observed output assumed to be generated by
\[
Y = h(X^{\text{(hid)}},U_\calD,U_Y)
\]
where $h$ is a deterministic function.

\item
$X_{U_\calI\leftarrow\tildeU_\calI}$: counterfactual variable to $X$ where $U_\calI$ has been replaced by $\tildeU_\calI$, i.e.,
\[
X_{U_\calI\leftarrow\tildeU_\calI} = T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}} \;.
\]

\item
Want CG-invariant representation
\[
\Gamma(X) = \Gamma(T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}) = \Gamma(T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}}) =  \Gamma(X_{U_\calI\leftarrow\tildeU_\calI})
\]
When $\calG_\calI$ is a normal subgroup of $\calG_{\calD\cup\calI}$, G-invariant representation
\[
\Gamma(X) = \Gamma(T_\calI\circ X)
\]
for all $T_\calI\in\calG_\calI$ is sufficient.

\item
Because groups are finite linear automorphisms, each transformation $T$ is just a linear function and so Reynolds operator can be applied directly to the group actions
\[
\bar{T} = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
For continuous linear groups, orbit-average over a Haar measure $\lambda$
\[
\bar{T} = \int_\calG T\lambda(T)
\]
\todo: need to estimate the operator. Assume uniform Haar and sample?

\end{itemize}

\section{Kernel hypothesis test notes}

\begin{itemize}

\item
Let $X$ be a r.v. on domain $\calX$. A kernel $k:\calX\times\calX\rightarrow\mathbb{R}$ induces a RHKS $\mathcal{H}$ of functions $f:\calX\rightarrow\mathbb{R}$ where for $f\in\mathcal{H}$ and $x\in\calX$,
\[
\langle f,k(x,\cdot)\rangle = f(x)
\]
(reproducing property). $k(x,\cdot)=\phi_x$ can also be considered an implicit feature map ($\phi_x:\calX\rightarrow\mathbb{R}$) where
\[
\langle \phi_x,\phi_{x'}\rangle = k(x,x')
\]
is a measure of similarity.

\item
Kernel methods work on inner products of feature maps of observations in the RKHS associated with kernel. Inner products may be computed without explicitly computing the high-dimensional feature map (``kernel trick'').

\item
Main challenge of designing kernel-based hypothesis tests is deriving large-sample distribution of test statistic under null.

\item
Gram matrix should be positive semidefinite. Satisfised if kernel is symmetric and positive semidefinite.

\item
Let $X$ be a r.v. with distribution $\mathbb{P}$. \href{https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions}{Mean element}
\[
\mu_\mathbb{P} = \mathbb{E}_{\mathbb{P}}[\phi_X]
\]
associated with $X$ is unique element of RKHS $\mathcal{H}$ s.t. for all $f\in\mathcal{H}$,
\[
\langle\mu_\mathbb{P},f\rangle = \mathbb{E}_\mathbb{P}[f(X)]
\]
Covariance operator $\Sigma_\mathbb{P}:\mathcal{H}\times \mathcal{H}\rightarrow\mathbb{R}$ associated with $X$ is unique operator s.t. for all $f,g\in\mathcal{H}$,
\[
\langle f, \Sigma_\mathbb{P}g\rangle = \mathrm{Cov}(f(X),g(X)) = \mathbb{E}_\mathbb{P}[f(X),g(X)] - \langle\mu_\mathbb{P},f\rangle\langle\mu_\mathbb{P},g\rangle
\]
Empirical estimates of inner products that lead to estimates of element/operator are available.

\item
Kernel is characteristic if mean embedding $\mu:\mathbb{P}\rightarrow\mathcal{H}$ is injective. Each distribution can be uniquely represented in the RKHS and all statistica features of distributions are preserved (\todo) by a characteristic kernel.

\item
If $\mathrm{dim}(\mathcal{H})=\infty$, $\mu_\mathbb{P}$ has more significance than in classical statistics.

\item
(Kellner,2015) MMD(? or related quantity) is a pseudo-metric. If restricting space of functions (e.g., r.v. space) to unit ball of RKHS with positive semi-definite characteristic kernel, MMD is a metric.

\end{itemize}

\section{Detecting conditional invariances in single training environment via hypothesis testing}

\begin{itemize}

\item
Case of invariance of output to single known group acting on inputs. Following (Mouli, 2021) in that we specify potential group of invariant transformations and testing if dataset contradicts invariance to the group, rather than if dataset appears to be invariant to group.
\\

\todo Contradict in hard sense (e.g., certain transformation impossible) or distributional sense (e.g., certain transformations unlikely)?

\item
Other assumptions?

\item
Possible two-sample test procedure: generate transformed inputs as ``second'' sample and compare conditional mean embeddings (e.g., Gretton, 2007). How should embeddings be estimated? (Song, 2013) Conditional embedding operator is a family of points in RKHS. Only when conditioned on a fixed value is it a single point.
\\

Given dataset $\{(x_i,y_i)\}_{i=1}^N$ assumed to be from some joint distribution $\mathbb{P}_{XY}=\mathbb{P}_X\mathbb{P}_{Y|X}$, want to determine if $\mathbb{P}_{Y|X}\overset{d}{=}\mathbb{P}_{Y|\calG\cdot X}$ for group $\calG$ where $\mathbb{P}_{Y|\calG\cdot X}(y|x)=\mathbb{P}_{Y|\calG\cdot X}(y|g\cdot x)$ for all $g\in\calG$, $x\in\calX$. (Elesedy, 2021) obtains invariant functions by orbit-averaging. We want a metric of some form \todo
\[
\mathrm{MMD}(\mathbb{P},\mathbb{P}') = \|\mu_{Y|X} - \mu_{Y|\calG\cdot X}\|_\mathcal{H}^2
\]
How to handle $X$ and $\mu_{Y|X}$ being a family of functions? $\mu_{Y|\calG\cdot X}$ estimated by generating/summing/integration transformed inputs?
\\

Comparing joint distributions only makes sense if it is assumed $\mathbb{P}(X)=\mathbb{P}(\calG\cdot X)$? This does not make sense for extrapolation context ($Y|X$ does not have to be invariant then?).

\item
If conditional is invariant to group, then conditional embedding should be same for orbit of conditioned value. Does that imply anything for conditional embedding operator? Projection? Invariant conditional embedding operator conditioned on orbits?

\item
Hard contradictions vs soft contradictions?

\item
One-sample test? Via parametric bootstrap (Kellner, 2015)?

\end{itemize}

\todo

Context 1:

\begin{itemize}

\item
Assume that we have a dataset $\calD=\{(x_i,y_i)\}_{i=1}^n$, $x_i\in\calX$, $y_i\in\calY$, from a single environment, i.e., $x_i\sim\bbP_X$ and $y_i\sim\bbP_{Y|X}$. Let $\bbP_{XY}=\bbP_X\bbP_{Y|X}$ be the joint distribution corresponding to that environment.

\item
Assume that the differences in environments can be modeled by some unknown group $\calG$ acting on inputs $X\in\calX$, e.g., $\bbP(X)$ and $\bbP(g\cdot X)$ for $\mathrm{id}()\neq g\in\calG$ are not equal and correspond to different environments. Assume that there is a true $\bbP_{Y|X}$ that holds across environments.

\item
We try to determine if $\bbP_{Y|X}$ being invariant to transformations $g\in\calG$ on $X$ for some specified $\calG$ would contradict the dataset, i.e., whether $\bbP_{Y|X}\equdist\bbP_{Y|g\cdot X}$ is plausible for $\calD$.

\item
Proposed approach 1: bootstrap samples from different environments by applying transformations to $X$ then do kernel hypothesis test (via MMD) to compare conditional embeddings. \todo Problem1: if  $\bbP(X)$ and $\bbP(g\cdot X)$ are not equal, how to bootstrap sample from a different environment? \todo Problem2: even if invariance does not ``contradict'', embedded distributions may be far apart? \todo Problem3: how to compare conditional embedding operator?

\end{itemize}

Context 2: observed $g$

\begin{itemize}

\item
Assume that we have a dataset $\calD=\{(x_i,g_i,y_i)\}_{i=1}^n$, $x_i\in\calX$, $y_i\in\calY$, $g_i\in\calG$ for some known group $\calG$ acting on $\calX$, from a single environment, i.e., $x_i\sim\bbP_X$, $g_i\sim\bbP_G$, $y_i\sim\bbP_{Y|X,G}$. Let $\bbP_{XYG}=\bbP_X\bbP_G\bbP_{Y|X,G}$ be the joint distribution corresponding to that environment. Examples of when $g$ is observed: image orientation, image background.

\item
Assume that $\bbP_G$ differs between environments.

\item
We try to determine if $\bbP_{Y|X,G}=\bbP_{Y|X}$, i.e., if $\mathbb{P}_{XYG}=\bbP_X\bbP_{Y|X}\bbP_G$. \todo This can be done through permutation-based kernel conditional independence tests?

\item
\todo If non-diverse $g$ observed, should not be a problem because does not contradict (test not rejected).

\end{itemize}

Context 3:

\begin{itemize}

\item
Assume that we have a dataset $\calD=\{(x_i,g_i,y_i)\}_{i=1}^n$, $x_i\in\calX$, $y_i\in\calY$, $g_i\in\calG$ for some known group $\calG$ acting on $\calX$, from a single environment, i.e., $x_i\sim\bbP_X$, $g_i\sim\bbP_G$, $y_i\sim\bbP_{Y|X,G}$. Let $\bbP_{XY\calG}=\bbP_X\bbP_G\bbP_{Y|X,G}$ be the joint distribution corresponding to that environment. Examples of when $g$ is observed: image orientation, image background.

\item
Assume that $\bbP_G$ differs between environments.

\item
We try to determine if $\bbP_{Y|X,G}=\bbP_{Y|X}$. \todo By Lemma~1 of \parencite{Elesedy:2021}, $\bbP_{XY}$ is invariant if and only if $\bbP_{XY}\equdist\int_\calG \bbP_{XYG}d\lambda$. We can then do standard two-sample kernel tests where second sample is obtained by orbit-averaged? \todo This is testing if distributions are consistent, not whether invariance contradicts.

\end{itemize}

\todo two-sample approach involving original and predicted datasets with invariant predictor?

\end{document}