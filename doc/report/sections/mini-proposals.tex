% !TEX root = ../main.tex

% Mini-proposals section

\section{Mini-proposals}

% each mini-proposal gets its own subsection
\subsection*{Proposal 1: Learning counterfactual G-invariances from single environments via multiple kernel learning}

The domain adaptation learning framework proposed by \textcite{Mouli:2021} can be restrictive in that the specified groups are required to be finite. Furthermore, neural networks, while powerful for prediction, can also be challenging to work with if interpretability is desirable or if there is available domain-knowledge to incorporate. We propose an adaptation to their framework based on multiple kernel learning \parencite{Gonen:2011} that addresses these (and potentially other) restrictions. Such a framework would also have access to the additional benefits that kernels may have to offer, such as being able to use specially-designed kernels and a potentially infinite-dimensional feature space. We note that the details described in this proposal were considered as part of the conceptual planning for the project and may be subject to change.
\\

Our proposed adaptation has the same goal as \citeauthor{Mouli:2021}'s framework, which is to be able to extrapolate a model trained on single-environment data to different environments by learning invariances for a subset of given groups that describe non-causal information. The SCM setup and assumptions in our proposed adaptation is mostly identical to that of the original framework. However, unlike the original framework, we allow for continuous groups $\calG_1,\ldots,\calG_m$ at the expense of restricting the output space $\calY=\bbR$. We also assume that the groups are compact to ensure the existence of Haar measures $\lambda_1,\ldots,\lambda_m$. We retain the assumption that the non-causal subset of groups $\calG_\calI$ is a normal subgroup of the overgroup $\calG_{\calD\cup\calI}$ to take advantage of Theorem~2 in \parencite{Mouli:2021}, which allows us to work directly with G-invariances rather than CG-invariances.
\\

The main difference between the original framework and our proposed adaptation is the model itself. While \textcite{Mouli:2021} propose to learn the invariances by encoding their respective invariant eigenspaces into neuron weights in a neural network layer, we propose to encode invariances as distinct invariant kernels in a prediction function that takes the form
\[
f(x) = \alpha_0 + \sum_{j=1}^p\alpha_j\sum_{i=1}^n\beta_i\bar{k}_j(x,x_i)
\]
where $p\approx 2^m$, $\bar{k}_j$ are kernels constructed to be invariant to different subsets of groups, $\beta_i$ are learned weights on the training data $x_i$, and $\alpha_j$ are learned weights on the kernels. Learning the invariances then corresponds to learning the weights $\alpha$ of the kernels in the model. The optimization objective in the adaptation still uses a regularization term that encourages a greater weight on the strongest invariances that do not contradict the training data.
\\

The main tasks of this proposed project would include the following:
\begin{enumerate}

\item
Show how to construct a kernel that is invariant to a specific set of groups. \textcite{Haasdonk:2005} show that for a given kernel $k:\calX\times\calX\rightarrow\bbR$ and a group $\calG$, integrating the kernel inputs over the (normalized) Haar measure $\lambda$ of the group produces the $\calG$-invariant kernel
\[
\bar{k}(x,x') = \int_\calG \int_\calG k(gx,g'x')\mathrm{d}\lambda(g)\mathrm{d}\lambda(g')\;.
\]
However, given a set of groups $\calG_\calI$ where $\calI\subset\{1,\ldots,m\}$, it may not be obvious how to construct a function that is invariant to only transformations $g\in\calG_\calI$ and not $g'\in\calG_{\supset\calI}$ where $\calI$ is a strict subset of $\supset\!\calI$. \textcite{Mouli:2021} deal with this problem in the context of finite linear groups by computing the intersection of the invariant eigenspaces for the groups in a set and removing their projection onto invariant eigenspaces for overgroups (Theorem~3). We expect that a kernel invariant to a specific set of groups may be constructed (or at least described) by drawing on summation properties of reproducing kernel Hilbert spaces (RKHS) as well as RKHS decomposition results from \parencite{Elesedy:2021} (Lemmas~3 and 4 in particular). The goal would be to provide theoretical results analogous to Lemma~2 and Theorem~3 of \parencite{Mouli:2021}.

\item
If (1) is not possible or requires unreasonable assumptions, then identify the key obstacles that prevent the construction of such a kernel.

\item
Formulate a kernel-based version of the framework by \citeauthor{Mouli:2021}. The main technical considerations would be computing the invariant kernels needed (which depends on the feasibility of (1)), adapting the regularization term in the optimization objective to work with kernel weights rather than neuron weights, and designing a feasible algorithm for learning the weights.

\item
Determine the tractability of learning and using a model of the form given above. The value $p$ in the above function is the number of subsets in the power set of the groups, and $n$ is the number of training examples. This implies that just using the given model is of order $O(2^mn)$. The algorithm used by \textcite{Mouli:2021} is also exponential, but early termination conditions are included and so it is unclear how rarely the worst-case runtime occurs in practice. It remains to be seen whether similar strategies can be applied for our proposed model (e.g., excluding kernels for low-order subsets).

\end{enumerate}

If the tasks described above are completed successfully, the expected major contributions of this project would be as follows:

\begin{enumerate}

\item
Further development of invariant kernel theory. Formal theory regarding the construction of a kernel that is invariant to exactly a specific set of groups does not seem to have been explored in existing literature and is an interesting idea in its own right.

\item
A CG-invariance learning framework for single-environment domain adaptation based on kernels that works with (possibly non-linear) continuous groups. In addition to being a potentially useful method, understanding whether other methods aside from the one proposed by \citeauthor{Mouli:2021} are feasible is critical for drawing attention to the relatively new single-environment domain adaptation literature.

\item
Empirical results that evaluate how the adapted framework compares to standard domain adaptation methods and the one proposed by \citeauthor{Mouli:2021}.

\end{enumerate}

\iffalse
\begin{itemize}

\item
Setting: $\calY=\bbR$, $\calG_1,\ldots,\calG_m$ compact with Haar measures $\lambda_1,\ldots,\lambda_m$, $\calG_\calI\trianglelefteq\calG_{\calD\cup\calI}$

\item
Idea: adapt original algorithm to use kernel regression rather than neural networks. Instead of finding bases of eigenspaces and learning the coefficients to learn invariances, do multiple kernel regression where the kernels are invariant to actions of subsets of groups and learn the invariances by learning the weights

\item
Contributions if successful: development of invariant kernel theory (constructing kernels invariant to a specific set of groups and no more), CG-invariance learning framework that allows for potentially infinite dimensional feature space and (possibly non-linear) continuous groups

\item
Main tasks:
\begin{enumerate}
\item
Show how to construct kernels that are invariant to multiple groups and not invariant to overgroups (if possible)
\item
Formalize theory under which construction holds (analogous to Lemmas 1, 2 and Theorem 3 for original paper) or why not possible
\item
Introduce kernel-version of framework and adapt optimization objective for kernel regression (if 1 and 2 successful)
\end{enumerate}

\end{itemize}
\fi

\iffalse
\subsection{Proposal 1: Learning Counterfactual G-invariances from Single Environments with Continuous Linear Groups}

\begin{itemize}

\item
Finite groups $\rightarrow$ continuous linear groups, e.g., rotations, with assumptions to guarantee existence of Haar measure (e.g., locally compact?)

\item
(Elesedy 2021) looks at a RKHS and decomposes it into a space of G-invariant functions and a space of functions that vanish when averaged over G.

\item
Reynolds operator is a functional(?) of a group:
\[
\bar{T}_\calG = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
Orbit-averaging operator is a functional(?) of a group and a function (the representation?):
\[
\calO_{\calG,x}(f) = \int_\calG f(gx)d\lambda(g)
\]
How are these related? $\calO$ is an orthogonal projection on RKHS. Does the eigenspace of $\calO$ play a similar role to the eigenspace of Reynolds operator?

\item
Assuming orbit-averaging replaces Reynolds operator, question is how to compute the 1-eigenspaces in practice (Reynolds operator can be computed for finite group, but averaging via Haar measure?).

\item
Results analogous to Lemma 1, 2 (1-eigenspace corresponds to invariant vectors) and Theorem 3 (subspaces for sets of groups) would be needed.

\end{itemize}


% each mini-proposal gets its own subsection
\subsection{Proposal 2: Learning Counterfactual G-equivariances from Single Environments}

\begin{itemize}

\item
Context: same as \parencite{Mouli:2021}, but equivariance rather than invariance is desirable (e.g., orientation of image).

\item
Idea: 0-eigenspace corresponds to equivariant aspect of group. Transform input to group representation (basis?), feed into invariant representation, re-apply transformation to obtain equivariant output.

\item
Questions: how to handle multiple groups? Input and output spaces potentially different, need to learn input transformation and output transformation separately?

\item
The intersection of 0-eigenspace for group $i$ and 1-eigenspace for groups $j\neq i$ (and remove orthogonal projection onto overgroups?) correspond to space of vectors that are invariant to actions of groups $j$ but not group $i$. For a set of groups of size $k$, we end up with $k$ different subspaces? Or intersect with 1-eigenspace of all other groups to only get 1 subspace for set?

\item
Input transformation: just use projection on 1-eigenspace and feed into invariant representation.

\item
?Output transformation: want to use projection on 0-eigenspace (intersected with other 1-eigenspaces), but representation is on different space than input? Can this output transformation be learned using the same framework? Or just have two separate input network components that each take one of projection onto 0/1-eigenspace and sum the outputs/feed both into link function?

\end{itemize}
\fi

% ...


% TODO
