% !TEX root = ../main.tex

% Summary section

\section{Conceptual summary}

The paper by \textcite{Mouli:2021} examines the problem of extrapolating patterns learned from single-environment training data in a supervised setting to data from other environments. This problem context falls under the topic of \textit{domain adaptation} that has been explored in recent literature \parencite{Farahani:2020}. However, a key assumption in \citeauthor{Mouli:2021}'s work that distinguishes it from previous work in the literature is that the training data come from a single environment as opposed to multiple environments. Several previously proposed methods for domain adaptation---such as \textit{Invariant Risk Minimization} \parencite{Arjovsky:2020} (IRM)---rely on training data from multiple environments and therefore would fail under this problem context. \citeauthor{Mouli:2021} take a different approach by viewing extrapolation as counterfactual reasoning in a specified structural causal model (SCM) and assuming that potential differences between environments can be described in terms of known (linear automorphism) transformation groups acting on the data. Under this formulation, \citeauthor{Mouli:2021} introduce a framework for the single-environment problem that is able to learn the group invariances that do not contradict the data. In this conceptual summary, we review the key contributions of the paper by \textcite{Mouli:2021} and discuss the strengths and weaknesses of their approach.


\subsection{Key differences from previous work}

Various methods for domain adaptation have been proposed in the literature, but the majority of these methods are not appropriate for the single-environment problem described by \textcite{Mouli:2021}. For example, existing causal-based methods such as IRM and \textit{Independent Causal Mechanisms} \parencite{Parascandolo:2018} (ICM) generally involve learning some internal representation of the data that is invariant to non-causal environment information. The invariance in the representation is learned from the training data, which is assumed to come from multiple environments. When the data come from a single environment, the representation cannot distinguish which aspects of the data are environment-specific and so the learned representation is unlikely to extrapolate to new environments. The learning framework proposed by \citeauthor{Mouli:2021} works with single-environment data and has an advantage over existing methods in these settings.
\\

Another well-known approach to domain adaptation is based on data augmentation \parencite{Chen:2020} where training is done with not only the original data but also proper transformations of the data. By augmenting the training data with seemingly irrelevant transformations, the aim is to desensitize the representation to these transformations and therefore learn invariance. \citeauthor{Mouli:2021} explain that data augmentation is a type of \textit{forced group invariance} (i.e., forced \textit{G-invariance}) where certain transformations of the data may actually introduce contradictions (e.g., trying to enforce rotation invariance in images of digits, but digits 6 and 9 are not invariant to 180$^o$ rotations). Like in data augmentation, \citeauthor{Mouli:2021}'s proposed framework starts with an a priori set of potential invariances (in the form of known groups rather than data), but the framework differs in that it then ``unlearns'' the invariances that contradict the training data.


\subsection{Main contributions}

The main contributions of the paper by \textcite{Mouli:2021} include a formulation of the single-environment extrapolation problem, a learning framework based on the formulation that aims to learn the non-contradicting invariances, and an empirical evaluation of standard neural networks versus neural networks trained using the proposed learning framework.
\\

\citeauthor{Mouli:2021}'s formulation of the single-environment extrapolation problem is based on the ICM literature where a SCM describes the input environment variables that influence or are irrelevant to the internal representation (\todo cite?). Extrapolation is then viewed as counterfactual reasoning where being able to extrapolate to different environments is tied to the representation being invariant to interventions on non-causal environment variables. \citeauthor{Mouli:2021} extend this idea by assuming that differences between environments can be described in terms of known linear automorphism groups that act on the variables. Being able to extrapolate a representation is then equivalent to the representation being counterfactually invariant (i.e., \textit{CG-invariant}) to the groups that act on non-causal variables. This additional assumption is the crux of the formulation that allows the proposed framework to work with only single-environment data.
\\

The learning framework aims to learn an internal representation that is CG-invariant to the groups that do not contradict the training data. While G-invariances are easier to work with in practice, \citeauthor{Mouli:2021} show that CG-invariance is stronger than G-invariance (Theorem 1). However, they also show that when the subset of groups acting on the non-causal variables is a normal subgroup of the overgroup acting on all variables, then G-invariance also implies CG-invariance (Theorem 2). These results establish the conditions under which it is sufficient for the model to learn G-invariances in place of CG-invariances, and it is due to these results that \citeauthor{Mouli:2021} also assume that the subgroup acting on non-causal variables is normal to the overgroup on all variables.
\\

The challenge in learning the G-invariances that do not contradict the training data is due to the fact that the subset of non-causal variables among all variables is unknown. To learn the invariances for the unknown set, \citeauthor{Mouli:2021} require the groups to be finite linear automorphisms. A group-invariant transformation---the \textit{Reynolds operator}---can then be constructed by averaging over members of the particular group (Lemma 1). For linear transformations, the Reynolds operator is a projection operator with eigenvalues 1 and 0. The left eigenspace spanned by eigenvectors with eigenvalue 1 represents the space of vectors that are invariant to transformations of the group (Lemma 2). \citeauthor{Mouli:2021} exploit this property to construct the subspace that is invariant to transformations of a set of groups, which is obtained by taking the intersection of the 1-eigenspaces for all groups in the set (Theorem 3). The invariant subspace is computed for each set in the power set of groups, and the invariant subspaces are partially ordered by their invariance \textit{strength} (i.e., the number of groups that the subspace is invariant to). The framework's optimization objective then includes a regularization term that encourages the learned representation to have the strongest G-invariance that does not significantly contradict the data. The key aspects of \citeauthor{Mouli:2021}'s proposed framework include needing to specify known groups, requiring the groups to be linear automorphisms and, in doing so, being able to learn the G-invariances that do not contradict the data.
\\

\citeauthor{Mouli:2021} evaluated neural networks trained using their proposed learning framework on various image tasks and array tasks. Their results broadly suggest that
\begin{enumerate}

\item
standard neural networks do well when interpolating but not when extrapolating,

\item
neural networks trained with forced G-invariances do poorly when interpolating but do well when extrapolating, and

\item
neural networks trained with their learning framework generally do well when interpolating and when extrapolating.

\end{enumerate}

 
\subsection{Limitations}

The main limitations of the framework proposed by \textcite{Mouli:2021} are the very specific assumptions required for the framework to work. To allow for single-environment data, the framework requires that the invariance groups acting on the data are known. Furthermore, to enable automatic learning of invariances that do not contradict the training data, the groups are also restricted to be finite linear automorphisms. Invariance groups that were not initially specified are also unable to be learned. The framework cannot be used if the differences between environments could not be expressed in terms of linear transformation groups that act on the data.
\\

These limitations naturally point to future work in the form of an extended framework that allows one or more of the assumptions to be violated. Other possible directions worth investigating include an equivariant version of the framework and a more efficient algorithm.


\newpage


\section{Technical summary}

The technical components of the paper by \textcite{Mouli:2021} include the proposed learning framework and the theoretical results that justify its usage in the given setting. In this technical summary, we introduce the formulation and notation of the single-environment extrapolation problem, discuss the assumptions made in the formulation, and describe the proposed learning framework for use under the formulated setting.


\subsection{Single-environment extrapolation setting}

In the context of single-environment extrapolation described by \textcite{Mouli:2021}, the goal is to learn (in a supervised learning setup) a prediction model where the output  only depends on information that is relevant across different environments. The challenge is learning which information is relevant given only training data from a single environment. To simplfy this problem, \citeauthor{Mouli:2021} assume a known set of groups $\calG_1,\ldots,\calG_m$ of linear transformations acting on the data, and the objective is to learn an internal representation of the data that is invariant to an unknown subset of groups assumed to be irrelevant to the output. \citeauthor{Mouli:2021} works under an ICM setup where the SCM in Figure~\ref{fig:scm} is assumed.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{scm.png}
\caption{Structural causal model assumed in single-environment extrapolation. Grey nodes are observed variables. Figure taken from \parencite{Mouli:2021}.}
\label{fig:scm}
\end{figure}

The variables in the assumed SCM are defined as follows:
\begin{itemize}

\item
$\calD$, $\calI$: (unknown) disjoint sets of indices that describe the groups of transformations that are relevant and irrelevant to the output, respectively. $\calD\cup\calI=\{1,\ldots,m\}$.

\item
$U_Y$, $U_\calI$, $U_\calD$, $\tildeU_\calI$: independent latent variables that influence the value of the variable(s) that they point to.

\item
$X^{\text{(hid)}}$: some unknown canonical form of the observed input $X$.  It is assumed that given $U_\calD$ and $U_\calI$, $X$ was obtained from an ordered sequence of transformations on the canonical form, i.e.,
\[
X=T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}
\]
where transformations
\[
T_{U_\calD,U_\calI}=T_\calI^{(1)}\circ T_\calD^{(1)}\circ T_\calI^{(2)}\circ\ldots
\]
make up the overgroup $\calG_{\calD\cup\calI}$, $T_\calD^{(j)}$ is a transformation in group $\calG_j$ from the overgroup $\calG_\calD=\langle\cup_{j\in\calD}\calG_j\rangle$, and $T_\calI^{(i)}\in\calG_i\subset\calG_\calI=\langle\cup_{i\in\calI}\calG_i\rangle$. Note that $\calG_\calI$ is also assumed to be a normal subgroup of $\calG_{\calD\cup\calI}$.

\item
$Y$: observed output assumed to be generated by
\[
Y = h(X^{\text{(hid)}},U_\calD,U_Y)
\]
where $h$ is a deterministic function.

\item
$X_{U_\calI\leftarrow\tildeU_\calI}$: counterfactual variable to $X$ where $U_\calI$ has been replaced by $\tildeU_\calI$, i.e.,
\[
X_{U_\calI\leftarrow\tildeU_\calI} = T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}} \;.
\]

\end{itemize}

Given the SCM, the goal is to learn a representation $\Gamma:\calX\rightarrow\bbR^d$, $d\geq 1$, that is CG-invariant, i.e.,
\[
\Gamma(X) = \Gamma(X_{U_\calI\leftarrow\tildeU_\calI})
\]
where the equality implies $\Gamma(X_{U_\calI\leftarrow u})=\Gamma(X_{U_\calI\leftarrow u'})$ for all $u\in\supp(U_\calI)$, $u'\in\supp(\tildeU_\calI)$. The representation $\Gamma$ is fed into a learned link function $g:\bbR^d\rightarrow \text{Im} P(Y=y|X)$, $\text{Im} P(\argdot)$ being the image of $P(\argdot)$, which produces the prediction of the model, i.e.,
\[
\hat{Y}|X \sim g(\Gamma(X)) \;.
\]
For training data $X^\text{(tr)}$, if
\[
Y|X^\text{(tr)} \equdist \hat{Y}|X^\text{(tr)}\sim g_{\text{true}}(\Gamma_{\text{true}}(X^\text{(tr)}))
\]
and $\Gamma_\text{true}(X) = \Gamma_\text{true}(X_{U_\calI\leftarrow\tildeU_\calI})$, then $g_\text{true}\circ\Gamma_\text{true}$ extrapolates to test data $X^\text{(te)}$ in the sense that
\[
Y|X^\text{(te)} \equdist \hat{Y}|X^\text{(te)}\sim g_{\text{true}}(\Gamma_{\text{true}}(X^\text{(te)})) \;.
\]


\subsection{Assumptions in single-environment extrapolation}

\textcite{Mouli:2021} make a number of assumptions in the setup described in the previous section in order to simplify the extrapolation problem and to allow for a feasible learning framework. Compared to previous work in the literature, the unconventional assumptions involve the transformation groups acting on the data.
\\

Unlike previous work that assumes the availability of training data from multiple environments, the problem context considered by \citeauthor{Mouli:2021} specifically considers data from a single environment. Without additional information that suggests how data from different environments may differ, it is likely impossible to learn what pieces of information are environment-specific and irrelevant to the output. \citeauthor{Mouli:2021} get around this issue by assuming a priori knowledge of how environments may differ in the form of transformation groups. The assumed groups specify the potential ways data from different environments may differ, and it is left to the learning framework to ``unlearn'' the groups that contradict the training data.
\\

Furthermore, \citeauthor{Mouli:2021} assume that the subset $\calG_\calI$ of groups is a normal subgroup of the overgroup $\calG_{\calD\cup\calI}$. This assumption is a consequence of Theorems 1 and 2, which together state that CG-invariances are G-invariances, but G-invariances are CG-invariances only when $\calG_\calI\unlhd\calG_{\calD\cup\calI}$. The assumption is made as in practice, it is easier to work with G-invariances ($\forall T_\calI\in\calG_\calI$, $\Gamma(X)=\Gamma(T_\calI\circ X$)) than CG-invariances due to its simpler definition. The proof of Theorem~1 (CG-invariance $\Rightarrow$ G-invariance) relies on the fact that for any transformation $T_\calI\in\calG_\calI$, we can rewrite
\[
T_\calI\circ X = T_\calI\circ T_{U_\calD,U_\calI\leftarrow u}\circ X^\text{(hid)} = T_{U_\calD,U_\calI\leftarrow\tilde{u}}\circ X^\text{(hid)}
\]
where $T_{U_\calD,\argdot}\in\calG_{\calD\cup\calI}$ and $u,\tilde{u}\in U_\calI$. The result then follows from the definitions of CG-invariance and G-invariance for a representation $\Gamma$. To show that not all G-invariances are CG-invariances, the counterexample in Figure~\ref{fig:counterexample} is given. The proof of Theorem~2 (G-invariance $\Rightarrow$ CG-invariance when $\calG_\calI\unlhd\calG_{\calD\cup\calI}$) uses the fact that under the required assumption, any $T_\calD\circ T_\calI \in\calG_{\calD\cup\calI}$ can be written as $T_\calD\circ T_\calI=T_\calI'\circ T_\calD$ where $T_\calI,T_\calI'\in\calG_\calI$, $T_\calD\in\calG_\calD$. Showing CG-invariance given any sequence of transformations $T\in\calG_{\calD\cup\calI}$ then reduces to repeatedly rewriting the sequence with a leading $T_\calI'$ that is then removed using the G-invariance of the representation $\Gamma$. CG-invariance is then shown after only transformations from $\calG_\calD$ remain in the sequence. 

\begin{figure}[H]
\centering
\includegraphics[width=0.52\textwidth]{counterexample.png}
\caption{A counterexample for showing not all G-invariances are CG-invariances. The goal is to determine the orientation of an upright or flat rod in an image. A representation that sums the middle row of the image is (G-)invariant to horizontal translations of the image but not invariant to 90$^o$ rotations. Applying a translation before a rotation may result in a representation different from just applying a rotation, and so the representation is not CG-invariant. Figure taken from \parencite{Mouli:2021}.}
\label{fig:counterexample}
\end{figure}

While a set of transformation groups are assumed a priori, it is not known which groups actually influence the output. Additionally, some of the groups may specify invariances that contradict the training data. \citeauthor{Mouli:2021} deal with this problem in their learning framework by obtaining a partial ordering on the invariant subspaces in terms of their ``strength'' and encouraging learning of only the strongest invariances through a regularized objective. In order to identify the invariant subspaces and obtain a partial ordering, \citeauthor{Mouli:2021} require the transformation groups to be linear automorphisms. This restriction implies that the Reynolds operator given by
\[
\bar{T} = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
is a G-invariant projection operator (Lemma~1). The invariant subspace for a group $\calG$ is then precisely the left eigenspace corresponding to eigenvalue 1 of the Reynolds operator (Lemma~2). To construct the invariant subspace for a set of groups indexed by a set $M$ (Theorem~3), the intersection of the 1-eigenspaces for the individual groups is taken. However, the intersection may also contain vectors that are invariant to some overgroup, and so the group's projection onto the subspace of the overgroup is removed from the intersection. The subspace $\calB_M$ that remains describes the vectors that are invariant to transformations only from the set of groups that it was built from. (\todo proof) \citeauthor{Mouli:2021}'s learning framework relies on being able to identify the invariant subspace for every subset of groups. Once the subspaces have been identified, a cost can be assigned to each subspace that encourages adopting the subspace invariant to the most number of groups.


\subsection{Learning framework for single-environment extrapolation}

Suppose that $\calG_1,\ldots,\calG_m$ are known linear automorphisms. Under the context and assumptions described in the previous sections, the framework proposed by \textcite{Mouli:2021} aims to learn a CG-invariant representation $\Gamma$ and a link function $g$ (both of which are neural networks). The representation $\Gamma$ is a neural network layer with $H\geq1$ neurons. The $h$-th neuron has the form
\[
\Gamma^{(h)}(x) = \sigma\left(x^T\left(\sum_{i=1}^B\bfB_{M_i}\bfom_{M_i,h}\right)+b_h\right)
\]
where $\sigma(\argdot)$ is a non-linear activation function, $b_h$ is a bias parameter, $\bfB_{M_i}$ is a matrix whose columns are the orthogonal basis of the invariant subspace $\calB_{M_i}$ built from the set of groups indexed by $M_i$, and $\bfom_{M_i,h}$ are the learnable parameters which correspond to the linear combination coefficients of the orthogonal basis. The parameters are collected in a neuron weight matrix
\[
\bfOm =
\begin{bmatrix}
\bfom_{M_1,1} & \ldots & \bfom_{M_1,H} \\
\vdots & \ddots & \vdots \\
\bfom_{M_B,1} & \ldots & \bfom_{M_B,H}
\end{bmatrix}
\]
where $M_1,\ldots,M_B$, $B\leq \mathrm{dim}(\calX)$, are sets of indices corresponding to different subsets of groups. The optimization objective is then
\[
\hatOm,\hatb = \argmin_{\bfOm,\bfb}\sum_{(y^\text{(tr)},x^\text{(tr)})\in\calD^\text{(tr)}}\calL\left(y^\text{(tr)},g(\Gamma(x^\text{(tr)};\bfOm,\bfb))\right)+ \lambda R(\bfOm)
\]
where $\lambda>0$ is a regularization parameter, $R(\argdot)$ is the regularization penalty given by
\[
R(\bfOm) = \left|\{M_i:|M_i|>\ell, 1\leq i\leq B\}\right| + \sum_{i:|M_i|=\ell,1\leq i\leq B}\onevec\{\|\bfom_{M_i,\argdot}\|_2^2>0\}\;,
\]
and $\ell = \min\{|M_i|\cdot\onevec\{\|\bfom_{M_i,\argdot}\|_2^2>0\}: 1\leq i \leq B\}$. The number $\ell$ describes the smallest size across all sets of groups $M_i$ that are used by at least one neuron. The penalty $R(\argdot)$ then counts the number of sets that are larger or are equal in size to the smallest set. This objective encourages $\Gamma$ to use a subspace that is invariant to more groups. Note that while $R(\argdot)$ is discrete, a differentiable approximation is available for optimization.
\\

To use the learning framework, the subspaces for the power set of groups must first be computed. While the procedure only needs to be run once for a particular set of groups, the runtime is technically exponential as the subspace needs to be computed for every set in the power set. The procedure can be set to terminate early once a subspace equal in size to the space of the input is found, and \citeauthor{Mouli:2021} comment that it is unclear if the worst-case runtime occurs in practice.

\subsection{Analysis}

\todo
\parencite{Mouli:2021} requires specifying known linear groups of transformations.
\begin{itemize}

\item
What implications are there with infinite linear groups? Learning framework finds invariant subspace of group using properties (idempotency) of projection operator.
\begin{itemize}
\item
Reynolds operator changes to integral over normalized Haar measure of (locally compact) group? 
\item
Provably Strict Generalisation Benefit for Invariance in Kernel Methods (Elesedy 2021). Lemma 3: RKHS decomposes into space of G-invariant functions and space of functions that vanish when averaged over an orbit. Both spaces are RKHS with different kernels.
\item
Relation to normal subgroups and quotient spaces?
\end{itemize}

\item
If considering non-normal subgroups, then Theorem 2 does not hold. Have to work with CG-invariances rather than G-invariances? What exactly does this mean?

\item
Can the framework be adapted for some equivariant objective? i.e., learning equivariant groups rather than invariant groups.

\item
Framework breaks down with non-groups. Can a non-group approach work? Can non-group structures be specified/linear? Can a variant of Theorem 2 be derived?

\item
Can something be done without assuming known groups? Consider largest possible linear overgroup and automatically learn the strongest invariances?

\item
Does the chosen regularizing term make sense? Is it optimal? Does its differentiable approximation affect these properties?

\item
The proposed algorithm is exponential as it involves the power set. Can it be reduced? Appendix D: ``...the algorithm stops after finding all the basis for the space vec$(\calX)$, it is unclear if the worst-case runtime occurs in practice.''

\end{itemize}