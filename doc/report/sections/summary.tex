% !TEX root = ../main.tex

% Summary section

\section{Conceptual summary}

The paper by \textcite{Mouli:2021} examines the problem of extrapolating patterns learned from training data from a single environment in a supervised setting to data from other environments. This problem context falls under the idea of \textit{domain adaptation} that has been explored in recent literature \parencite{Farahani:2020}. However, a key assumption in \citeauthor{Mouli:2021}'s work that distinguishes it from previous work in the literature is that the training data come from a single environment as opposed to multiple environments. Several previously proposed methods for domain adaptation---such as \textit{Invariant Risk Minimization} \parencite{Arjovsky:2020} (IRM)---rely on training data from multiple environments and therefore would fail under this problem context. \citeauthor{Mouli:2021} take a different approach by viewing extrapolation as counterfactual reasoning in a specified structural causal model (SCM) and assuming known (linear automorphism) group structures on the non-causal mechanisms. Under this formulation, \citeauthor{Mouli:2021} introduce a learning framework for the single-environment context that is able to learn invariances that do not contradict the data. In this conceptual summary, we review the key contributions of the paper by \textcite{Mouli:2021} and discuss the strengths and weaknesses of their approach.


\subsection{Key differences from previous work}

Various methods for domain adaptation have been proposed in the literature, and how the work by \textcite{Mouli:2021} relates to these methods are highlighted in their paper. For example, methods based on causal inference such as IRM and \textit{Independent Causal Mechanisms} \parencite{Parascandolo:2018} (ICM) broadly involve learning some internal representation of the data that is invariant to environment non-causal mechanisms. The invariant representation is learned from the training data which come from multiple environments. When the data come from a single environment, the representation cannot determine which aspects of the data are environment-specific and so the representation is unlikely to extrapolate to new environments. The learning framework proposed by \citeauthor{Mouli:2021} works with single-environment data and has an advantage over existing methods in these settings.
\\

Another well-known approach to domain adaptation is based on data augmentation \parencite{Chen:2020} where training is done with not only the original data but also proper transformations of the data. \citeauthor{Mouli:2021} explains that data augmentation is a type of \textit{forced group invariance} (i.e., forced \textit{G-invariance}) where certain transformations of the data may actually introduce contradictions (e.g., trying to enforce rotation invariance in images of digits, but digits 6 and 9 are not invariant to 180$^o$ rotations). \citeauthor{Mouli:2021}'s proposed learning framework aims to learn only the invariances that do not contradict the training data.


\subsection{Main contributions}

The main contributions of the paper by \textcite{Mouli:2021} include a formulation of the single-environment extrapolation problem, a learning framework based on the formulation that aims to learn the non-contradicting invariances, and an empirical evaluation of standard neural networks versus neural networks trained using the proposed learning framework.
\\

\citeauthor{Mouli:2021}'s formulation of the single-environment extrapolation problem is based on the ICM literature where a SCM is used to describe the input variables that influence or are irrelevant to the internal representation across environments (\todo cite?). Extrapolation is then seen as counterfactual reasoning where being able to extrapolate to different environments is tied to the representation being invariant to interventions on non-causal environment variables. \citeauthor{Mouli:2021} extends this idea by assuming known linear automorphism groups acting on the non-causal variables, in which case extrapolation of a representation is equivalent to the representation being counterfactually invariant to a group (i.e., \textit{CG-invariant}). This extension is the crux of the formulation that allows the proposed learning framework to work with single-environment data.
\\

The learning framework proposed by \citeauthor{Mouli:2021} aims to learn an internal representation that is CG-invariant to specified groups that do not contradict the training data. \citeauthor{Mouli:2021} show that CG-invariance is stronger than G-invariance (Theorem 1), but when the group acting on the non-causal variables is a normal subgroup of the group acting on all variables, then G-invariance also implies CG-invariance (Theorem 2). These results establish the group conditions under which it is sufficient for the model to learn G-invariances in place of CG-invariances.
\\

The challenge in learning the G-invariances that do not contradict the training data is due to the fact that the set of non-causal variables among all variables is unknown. To learn the invariances for the unknown set, \citeauthor{Mouli:2021} require the groups to be (finite) linear automorphisms. A group-invariant transformation can be constructed by averaging over members of the group (the \textit{Reynolds operator}, Lemma 1). For linear transformations, the averaged transformation is a projection operator with eigenvalues 1 and 0. The left eigenspace spanned by eigenvectors with eigenvalue 1 represents the space of transformations that are invariant to the group (Lemma 2). \citeauthor{Mouli:2021} exploit this property by computing the intersection of these subspaces for all subsets of the set of groups, and the set of invariant subspaces can then be partially ordered by the size of their corresponding subset (i.e., the \textit{strength} of the invariance, Theorem 3). The objective optimized in the learning framework then includes a regularization term that encourages learning a representation with the strongest G-invariance that does not significantly contradict the data. The key aspects of this learning framework include needing to specify known groups, requiring the groups to be linear automorphisms and, in doing so, being able to automatically learn the G-invariances that do not (significantly) contradict the data.
\\

\citeauthor{Mouli:2021} evaluated neural networks trained using their proposed learning framework on various image tasks and array tasks. Their results broadly suggest that
\begin{enumerate}

\item
standard neural networks do well when interpolating but not when extrapolating,

\item
neural networks trained with forced G-invariances do poorly when interpolating but do well when extrapolating, and

\item
neural networks trained with their learning framework generally do well when interpolating and when extrapolating.

\end{enumerate}

 
\subsection{Limitations}

The main limitations of the learning framework proposed by \textcite{Mouli:2021} originate from the assumptions made. To allow for single-environment data, the framework requires that the invariance groups acting on the data are known and specified. Furthermore, to enable automatic learning of invariances that do not contradict the training data, the groups are also restricted to be linear automorphisms. This framework cannot be used if no linear group structure could placed on the transformations that act on the data.
\\

These limitations naturally point to directions for future work where non-linear groups or even non-groups of transformations are considered. It is also worth investigating if a method for single-environment extrapolation that does not require known groups is possible.


\newpage


\section{Technical summary}

The technical components of the paper by \textcite{Mouli:2021} include the proposed learning framework and the theoretical results that justify its usage in the given setting. In this technical summary, we introduce the formulation and notation of the single-environment extrapolation problem, discuss the assumptions made in the formulation, and describe the proposed learning framework for use under the formulated setting.


\subsection{Single-environment extrapolation setting}

In the context of single-environment extrapolation described by \textcite{Mouli:2021}, the goal is to learn (in a supervised learning setup) a prediction model where the output  only depends on information that is relevant across different environments. The challenge is learning which information is relevant given only training data from a single environment. To simplfy this problem, \citeauthor{Mouli:2021} assume a known set of groups $\calG_1,\ldots,\calG_m$ of linear transformations acting on the data, and the objective is to learn an internal representation of the data that is invariant to an unknown subset of groups assumed to be irrelevant to the output. \citeauthor{Mouli:2021} works under an ICM setup where the SCM in Figure~\ref{fig:scm} is assumed.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{scm.png}
\caption{Structural causal model assumed in single-environment extrapolation. Grey nodes are observed variables. Figure taken from \parencite{Mouli:2021}.}
\label{fig:scm}
\end{figure}

The variables in the assumed SCM are defined as follows:
\begin{itemize}

\item
$\calD$, $\calI$: (unknown) disjoint sets of indices that describe the groups of transformations that are relevant and irrelevant to the output, respectively. $\calD\cup\calI=\{1,\ldots,m\}$.

\item
$U_Y$, $U_\calI$, $U_\calD$, $\tildeU_\calI$: independent latent variables that influence the value of the variable(s) that they point to.

\item
$X^{\text{(hid)}}$: some unknown canonical form of the observed input $X$.  It is assumed that given $U_\calD$ and $U_\calI$, $X$ was obtained from an ordered sequence of transformations on the canonical form, i.e.,
\[
X=T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}
\]
where transformations
\[
T_{U_\calD,U_\calI}=T_\calI^{(1)}\circ T_\calD^{(1)}\circ T_\calI^{(2)}\circ\ldots
\]
make up the overgroup $\calG_{\calD\cup\calI}$, $T_\calD^{(j)}$ is a transformation in group $\calG_j$ from the overgroup $\calG_\calD=\langle\cup_{j\in\calD}\calG_j\rangle$, and $T_\calI^{(i)}\in\calG_i\subset\calG_\calI=\langle\cup_{i\in\calI}\calG_i\rangle$. Note that $\calG_\calI$ is also assumed to be a normal subgroup of $\calG_{\calD\cup\calI}$.

\item
$Y$: observed output assumed to be generated by
\[
Y = h(X^{\text{(hid)}},U_\calD,U_Y)
\]
where $h$ is a deterministic function.

\item
$X_{U_\calI\leftarrow\tildeU_\calI}$: counterfactual variable to $X$ where $U_\calI$ has been replaced by $\tildeU_\calI$, i.e.,
\[
X_{U_\calI\leftarrow\tildeU_\calI} = T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}} \;.
\]

\end{itemize}

Given the SCM, the goal is to learn a representation $\Gamma:\calX\rightarrow\bbR^d$, $d\geq 1$, that is CG-invariant, i.e.,
\[
\Gamma(X) = \Gamma(X_{U_\calI\leftarrow\tildeU_\calI})
\]
where the equality implies $\Gamma(X_{U_\calI\leftarrow u})=\Gamma(X_{U_\calI\leftarrow u'})$ for all $u\in\supp(U_\calI)$, $u'\in\supp(\tildeU_\calI)$. The representation $\Gamma$ is fed into a learned link function $g:\bbR^d\rightarrow \text{Im} P(Y=y|X)$, $\text{Im} P(\argdot)$ being the image of $P(\argdot)$, which produces the prediction of the model, i.e.,
\[
\hat{Y}|X \sim g(\Gamma(X)) \;.
\]
For training data $X^\text{(tr)}$, if
\[
Y|X^\text{(tr)} \equdist \hat{Y}|X^\text{(tr)}\sim g_{\text{true}}(\Gamma_{\text{true}}(X^\text{(tr)}))
\]
and $\Gamma_\text{true}(X) = \Gamma_\text{true}(X_{U_\calI\leftarrow\tildeU_\calI})$, then $g_\text{true}\circ\Gamma_\text{true}$ extrapolates to test data $X^\text{(te)}$ in the sense that
\[
Y|X^\text{(te)} \equdist \hat{Y}|X^\text{(te)}\sim g_{\text{true}}(\Gamma_{\text{true}}(X^\text{(te)})) \;.
\]


\subsection{Assumptions in single-environment extrapolation}

\textcite{Mouli:2021} make a number of assumptions in the setup described in the previous section in order to simplify the extrapolation problem and to allow for a feasible learning framework. Compared to previous work in the literature, the unconventional assumptions involve the transformation groups acting on the data.
\\

Unlike previous work that assumes the availability of training data from multiple environments, the problem context considered by \citeauthor{Mouli:2021} specifically considers data from a single environment. Without additional information that suggests how data from different environments may differ, it is likely impossible to learn what pieces of information are environment-specific and irrelevant to the output. \citeauthor{Mouli:2021} get around this issue by assuming a priori knowledge of how environments may differ in the form of transformation groups. The assumed groups specify the potential ways data from different environments may differ, and it is left to the learning framework to ``unlearn'' the groups that contradict the training data.
\\

Furthermore, \citeauthor{Mouli:2021} assume that the subset $\calG_\calI$ of groups is a normal subgroup of the overgroup $\calG_{\calD\cup\calI}$. This assumption is a consequence of Theorems 1 and 2, which together state that CG-invariances are G-invariances, but G-invariances are CG-invariances only when $\calG_\calI\unlhd\calG_{\calD\cup\calI}$. The assumption is made as in practice, it is easier to work with G-invariances ($\forall T_\calI\in\calG_\calI$, $\Gamma(X)=\Gamma(T_\calI\circ X$)) than CG-invariances due to its simpler definition. The proof of Theorem~1 (CG-invariance $\Rightarrow$ G-invariance) relies on the fact that for any transformation $T_\calI\in\calG_\calI$, we can rewrite
\[
T_\calI\circ X = T_\calI\circ T_{U_\calD,U_\calI\leftarrow u}\circ X^\text{(hid)} = T_{U_\calD,U_\calI\leftarrow\tilde{u}}\circ X^\text{(hid)}
\]
where $T_{U_\calD,\argdot}\in\calG_{\calD\cup\calI}$ and $u,\tilde{u}\in U_\calI$. The result then follows from the definitions of CG-invariance and G-invariance for a representation $\Gamma$. To show that not all G-invariances are CG-invariances, the counterexample in Figure~\ref{fig:counterexample} is given. The proof of Theorem~2 (G-invariance $\Rightarrow$ CG-invariance when $\calG_\calI\unlhd\calG_{\calD\cup\calI}$) uses the fact that under the required assumption, any $T_\calD\circ T_\calI \in\calG_{\calD\cup\calI}$ can be written as $T_\calD\circ T_\calI=T_\calI'\circ T_\calD$ where $T_\calI,T_\calI'\in\calG_\calI$, $T_\calD\in\calG_\calD$. Showing CG-invariance given any sequence of transformations $T\in\calG_{\calD\cup\calI}$ then reduces to repeatedly rewriting the sequence with a leading $T_\calI'$ that is then removed using the G-invariance of the representation $\Gamma$. CG-invariance is then shown after only transformations from $\calG_\calD$ remain in the sequence. 

\begin{figure}[H]
\centering
\includegraphics[width=0.52\textwidth]{counterexample.png}
\caption{A counterexample for showing not all G-invariances are CG-invariances. The goal is to determine the orientation of an upright or flat rod in an image. A representation that sums the middle row of the image is (G-)invariant to horizontal translations of the image but not invariant to 90$^o$ rotations. Applying a translation before a rotation may result in a representation different from just applying a rotation, and so the representation is not CG-invariant. Figure taken from \parencite{Mouli:2021}.}
\label{fig:counterexample}
\end{figure}

While a set of transformation groups are assumed a priori, it is not known which groups actually influence the output. Additionally, some of the groups may specify invariances that contradict the training data. \citeauthor{Mouli:2021} deal with this problem in their learning framework by obtaining a partial ordering on the invariant subspaces in terms of their ``strength'' and encouraging learning of only the strongest invariances through a regularized objective. In order to identify the invariant subspaces and obtain a partial ordering, \citeauthor{Mouli:2021} require the transformation groups to be linear automorphisms. This restriction implies that the Reynolds operator given by
\[
\bar{T} = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
is a G-invariant projection operator (Lemma~1). The invariant subspace for a group $\calG$ is then precisely the left eigenspace corresponding to eigenvalue 1 of the Reynolds operator (Lemma~2). To construct the invariant subspace for a set of groups indexed by a set $M$ (Theorem~3), the intersection of the 1-eigenspaces for the individual groups is taken. However, the intersection may also contain vectors that are invariant to some overgroup, and so the group's projection onto the subspace of the overgroup is removed from the intersection. The subspace $\calB_M$ that remains describes the vectors that are invariant to transformations only from the set of groups that it was built from. (\todo proof) \citeauthor{Mouli:2021}'s learning framework relies on being able to identify the invariant subspace for every subset of groups. Once the subspaces have been identified, a cost can be assigned to each subspace that encourages adopting the subspace invariant to the most number of groups.


\subsection{Learning framework for single-environment extrapolation}

Suppose that $\calG_1,\ldots,\calG_m$ are known linear automorphisms. Under the context and assumptions described in the previous sections, the framework proposed by \textcite{Mouli:2021} aims to learn a CG-invariant representation $\Gamma$ and a link function $g$ (both of which are neural networks). The representation $\Gamma$ is a neural network layer with $H\geq1$ neurons. The $h$-th neuron has the form
\[
\Gamma^{(h)}(x) = \sigma\left(x^T\left(\sum_{i=1}^B\bfB_{M_i}\bfom_{M_i,h}\right)+b_h\right)
\]
where $\sigma(\argdot)$ is a non-linear activation function, $b_h$ is a bias parameter, $\bfB_{M_i}$ is a matrix whose columns are the orthogonal basis of the invariant subspace $\calB_{M_i}$ built from the set of groups indexed by $M_i$, and $\bfom_{M_i,h}$ are the learnable parameters which correspond to the linear combination coefficients of the orthogonal basis. The parameters are collected in a neuron weight matrix
\[
\bfOm =
\begin{bmatrix}
\bfom_{M_1,1} & \ldots & \bfom_{M_1,H} \\
\vdots & \ddots & \vdots \\
\bfom_{M_B,1} & \ldots & \bfom_{M_B,H}
\end{bmatrix}
\]
where $M_1,\ldots,M_B$, $B\leq \mathrm{dim}(\calX)$, are sets of indices corresponding to different subsets of groups. The optimization objective is then
\[
\hatOm,\hatb = \argmin_{\bfOm,\bfb}\sum_{(y^\text{(tr)},x^\text{(tr)})\in\calD^\text{(tr)}}\calL\left(y^\text{(tr)},g(\Gamma(x^\text{(tr)};\bfOm,\bfb))\right)+ \lambda R(\bfOm)
\]
where $\lambda>0$ is a regularization parameter, $R(\argdot)$ is the regularization penalty given by
\[
R(\bfOm) = \left|\{M_i:|M_i|>\ell, 1\leq i\leq B\}\right| + \sum_{i:|M_i|=\ell,1\leq i\leq B}\onevec\{\|\bfom_{M_i,\argdot}\|_2^2>0\}\;,
\]
and $\ell = \min\{|M_i|\cdot\onevec\{\|\bfom_{M_i,\argdot}\|_2^2>0\}: 1\leq i \leq B\}$. The number $\ell$ describes the smallest size across all sets of groups $M_i$ that are used by at least one neuron. The penalty $R(\argdot)$ then counts the number of sets that are larger or are equal in size to the smallest set. This objective encourages $\Gamma$ to use a subspace that is invariant to more groups. Note that while $R(\argdot)$ is discrete, a differentiable approximation is available for optimization.
\\

To use the learning framework, the subspaces for the power set of groups must first be computed. While the procedure only needs to be run once for a particular set of groups, the runtime is technically exponential as the subspace needs to be computed for every set in the power set. The procedure can be set to terminate early once a subspace equal in size to the space of the input is found, and \citeauthor{Mouli:2021} comment that it is unclear if the worst-case runtime occurs in practice.

\subsection{Analysis}

\todo is this required in the summary?