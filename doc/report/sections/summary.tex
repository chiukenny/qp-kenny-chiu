% !TEX root = ../main.tex

% Summary section

\section{Conceptual summary}\label{sec:conceptual}

The paper by \textcite{Mouli:2021} examines the problem of extrapolating patterns learned from single-environment training data in a supervised setting to data from other environments. This problem context falls under the topic of \textit{domain adaptation} that has been explored in recent literature \parencite{Farahani:2020}. However, a key assumption in \citeauthor{Mouli:2021}'s work that distinguishes it from much of the previous work in the literature is that the training data come from a single environment as opposed to multiple environments. Several previously proposed methods for domain adaptation---such as \textit{Invariant Risk Minimization} \parencite{Arjovsky:2020} (IRM)---rely on training data from multiple environments and therefore would fail under this problem context. \citeauthor{Mouli:2021} take a different approach by viewing extrapolation as counterfactual reasoning in a specified structural causal model (SCM) and assuming that potential differences between environments can be described in terms of known linear transformation groups acting on the data. Under this formulation, \citeauthor{Mouli:2021} introduce a neural network learning framework for the single-environment problem that is able to learn the group invariances that do not contradict the data. In this conceptual summary, we discuss how the context and work of \textcite{Mouli:2021} differ from previous work in the literature, review the key contributions of their work, and highlight the limitations of their approach.


\subsection{Related work}

Various methods for domain adaptation have been proposed in the literature, but the majority of these methods are not appropriate for the single-environment problem described by \textcite{Mouli:2021}. For example, methods based on the \textit{Independent Causal Mechanisms} (ICM) principle \parencite{Parascandolo:2018} and other causal-based methods are generally based on learning some internal representation of the data that is invariant to non-causal environment information. The invariance in the representation is learned from the training data, which is assumed to come from multiple environments. When the data come from a single environment, the representation cannot distinguish which aspects of the data are environment-specific and so the learned representation is unlikely to extrapolate to new environments. The learning framework proposed by \citeauthor{Mouli:2021} works with single-environment data and has an advantage over existing methods in these settings.
\\

Another common approach to domain adaptation is based on data augmentation \parencite{Chen:2020} where training is done with not only the original data but also proper transformations of the data. By augmenting the training data with seemingly irrelevant transformations, the aim is to desensitize the representation to these transformations and therefore learn invariance. \citeauthor{Mouli:2021} explain that data augmentation is a type of \textit{forced group invariance} (i.e., forced \textit{G-invariance}) where certain transformations of the data may actually introduce contradictions (e.g., trying to enforce rotation invariance in images of digits, but digits 6 and 9 are not invariant to 180$^o$ rotations). Like in data augmentation, \citeauthor{Mouli:2021}'s proposed framework starts with an a priori set of potential invariances (in the form of known groups rather than data), but the framework differs in that it then ``unlearns'' the invariances that contradict the training data.
\\

While the single-environment problem is not entirely novel in the domain adaptation literature, the context of the problem and the proposed approaches to solve it vary greatly across works. For example, \textcite{Kumar:2020} study reinforcement learning in the setting where only a single training Markov decision process is available. The \textit{single-source unsupervised domain adaptation} literature examines problems where labeled data is only available from a single source and labels for data from other sources have to be predicted \parencite{Zhao:2020}. \citeauthor{Mouli:2021}'s work fits into this literature but differs from most others in terms of its problem formulation and setup.


\subsection{Main contributions}

The main contributions of \textcite{Mouli:2021} include a formulation of the single-environment extrapolation problem, a learning framework for neural networks that aims to learn the non-contradicting invariances, and an empirical evaluation of standard neural networks versus neural networks trained using the proposed learning framework.
\\

\citeauthor{Mouli:2021}'s formulation of the single-environment extrapolation problem is based on the ICM principle and involves a SCM describes the causal and non-causal relationships between the variables \parencite{Scholkopf:2019}. Extrapolation is then viewed as counterfactual reasoning where being able to extrapolate to different environments is tied to the output being invariant to interventions on non-causal environment variables. \citeauthor{Mouli:2021} extend this idea by assuming that differences between environments can be described in terms of known linear automorphism groups that act on the variables. Being able to extrapolate a representation is then equivalent to the representation being counterfactually group-invariant (i.e., \textit{CG-invariant}) to the groups that act on non-causal variables. This additional assumption is the crux of the formulation that allows the proposed framework to work with only single-environment data.
\\

The learning framework aims to learn an internal representation that is CG-invariant to the groups that do not contradict the training data. While G-invariances are easier to work with in practice, \textcite{Mouli:2021} show that CG-invariance is stronger than G-invariance (Theorem 1). However, they also show that when the subset of groups acting on the non-causal variables is a normal subgroup of the overgroup acting on all variables, then G-invariance also implies CG-invariance (Theorem 2). These results establish the conditions under which it is sufficient for the model to learn G-invariances in place of CG-invariances, and it is for these reasons that \citeauthor{Mouli:2021} also assume that the subgroup acting on non-causal variables is normal to the overgroup on all variables.
\\

The challenge in learning the G-invariances that do not contradict the training data is due to the fact that the subset of non-causal variables among all variables is unknown. To learn the invariances for the unknown set, \citeauthor{Mouli:2021} require the groups to be finite linear automorphisms. The \textit{Reynolds operator}---a group-invariant transformation---can then be constructed by averaging over members of the particular group (Lemma 1). The Reynolds operator is a projection operator with eigenvalues 1 and 0. The left eigenspace spanned by eigenvectors with eigenvalue 1 represents the space of vectors that are invariant to transformations of the group (Lemma 2). To construct the subspace that is invariant to transformations of a specific set of groups, the intersection of the 1-eigenspaces for all groups in the set is taken, and the projection of the intersection onto the subspace of all overgroups is then removed from the intersection (Theorem 3). The invariant subspace is computed for each set in the power set of groups, and the invariant subspaces are partially ordered by their invariance strength (i.e., the number of groups that the subspace is invariant to). A basis for each subspace is then computed and encoded into a neural network where the learned parameters are neuron weights representing the coefficients for each basis. The framework's optimization objective then includes a regularization term that encourages the network representation to use the strongest G-invariance (i.e., have a non-zero weight) that does not significantly contradict the data, and to avoid invariances (i.e., have zero weights) that are lower-order or contradicting. The key aspects of \citeauthor{Mouli:2021}'s proposed framework include needing to specify known groups, requiring the groups to be finite linear automorphisms and, in doing so, being able to learn the G-invariances that do not contradict the data.
\\

\textcite{Mouli:2021} evaluated neural networks trained using their proposed learning framework on various image tasks and array tasks. Their results broadly suggest that
\begin{enumerate}

\item
standard neural networks do well when interpolating but not when extrapolating,

\item
neural networks trained with forced G-invariances do poorly when interpolating but do well when extrapolating, and

\item
neural networks trained with their learning framework generally do well when interpolating and when extrapolating.

\end{enumerate}
Based on these conclusions, there appears to be merit in their proposed framework, and their approach may be worth further exploring in future work.

 
\subsection{Limitations}

The main limitations of the framework proposed by \textcite{Mouli:2021} are the very specific assumptions required for the framework to work. To allow extrapolation of the model trained on single-environment data to different environments, the framework requires that the invariance groups acting on the data are specified a priori. Furthermore, to enable automatic learning of invariances that do not contradict the training data, the groups are restricted to be finite linear automorphisms. These restrictions imply that invariance groups that were not initially specified are unable to be learned. The framework also cannot be used if the differences between environments could not be expressed in terms of linear transformation groups that act on the data. These limitations naturally point to future work in the form of an extended framework that allows one or more of these assumptions to be violated.



\newpage



\section{Technical summary}

The main technical aspects of the paper by \textcite{Mouli:2021} include the proposed neural network learning framework and the theoretical results that justify its usage in the described problem setting. In this technical summary, we introduce the formulation and notation of the single-environment extrapolation problem, discuss the assumptions that are made and why, and explain how the proposed learning framework is used with a neural network.
\\

Note that the definitions of terms and acronyms used in this technical summary are defined in the conceptual summary in Section~\ref{sec:conceptual}.


\subsection{Single-environment extrapolation setting}

In the setting of single-environment extrapolation described by \textcite{Mouli:2021}, the goal is to train a prediction model (under a supervised learning setup) that is able to perform well (i.e., extrapolate) across different environments when given only data from a single environment at training time. It is assumed that input data include causal and non-causal variables in relation to the output, and that environments differ only in the non-causal variables. In theory, the model should be able to extrapolate if it depends only on the causal variables. The challenge is then learning which information is causal and which is not given only training data from a single environment. To simplify the problem, \citeauthor{Mouli:2021} assume that differences in the data can be described by a given set of finite linear transformation groups $\calG_1,\ldots,\calG_m$ acting on the data. The objective is then to determine which of these groups correspond to non-causal information and to learn an internal representation of the data that is invariant to these non-causal groups.
\\

\citeauthor{Mouli:2021}'s formulation of the problem follows the ICM principle \parencite{Parascandolo:2018} and assumes the SCM in Figure~\ref{fig:scm}.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{scm.png}
\caption{Assumed SCM in single-environment extrapolation. Grey nodes are observed variables. Arrows denote deterministic causal relationships where the target is dependent on the source. Figure from \parencite{Mouli:2021}.}
\label{fig:scm}
\end{figure}
\noindent The variables in the SCM in Figure~\ref{fig:scm} are defined as follows:
\begin{itemize}

\item
$\calD$, $\calI$: unknown disjoint sets of indices that refer to the sets of transformation groups that are relevant and irrelevant to the output, respectively. $\calD\cup\calI=\{1,\ldots,m\}$.

\item
$U_Y$, $U_\calI$, $U_\calD$, $\tildeU_\calI$: independent latent variables that model the stochastic components in the SCM. $U_\calI$ and $U_\calD$ in particular represent the collective randomness in their respective set of groups.

\item
$X^{\text{(hid)}}$: unknown canonical form of the observed input $X\in\calX$.  It is assumed that given $U_\calD$ and $U_\calI$, $X$ was obtained from an ordered sequence of transformations $T_{U_\calD,U_\calI}$ applied to the canonical form, i.e.,
\[
X=T_{U_\calD,U_\calI}\circ X^{\text{(hid)}}\;.
\]
The overgroup $\calG_{\calD\cup\calI}$ consists of transformations of the form
\[
T_{U_\calD,U_\calI}=T_\calI^{(1)}\circ T_\calD^{(1)}\circ T_\calI^{(2)}\circ\ldots
\]
where $T_\calD^{(j)}$ is a transformation in group $\calG_j$ from the overgroup $\calG_\calD=\langle\cup_{j\in\calD}\calG_j\rangle$, and similarly $T_\calI^{(i)}\in\calG_i\subset\calG_\calI=\langle\cup_{i\in\calI}\calG_i\rangle$. Note that \citeauthor{Mouli:2021} also assume that $\calG_\calI$ is a normal subgroup of $\calG_{\calD\cup\calI}$ (denoted $\calG_\calI\trianglelefteq\calG_{\calD\cup\calI}$).

\item
$Y$: observed output assumed to be generated by
\[
Y = h(X^{\text{(hid)}},U_\calD,U_Y)
\]
where $h$ is a deterministic function.

\item
$X_{U_\calI\leftarrow\tildeU_\calI}$: counterfactual variable to $X$ where $U_\calI$ has been replaced by $\tildeU_\calI$, i.e.,
\[
X_{U_\calI\leftarrow\tildeU_\calI} = T_{U_\calD,\tildeU_\calI}\circ X^{\text{(hid)}} \;.
\]

\end{itemize}

Given the SCM, the goal is to learn a representation $\Gamma:\calX\rightarrow\bbR^d$, $d\geq 1$, that is CG-invariant, i.e.,
\[
\Gamma(X) = \Gamma(X_{U_\calI\leftarrow\tildeU_\calI})
\]
where the equality implies $\Gamma(X_{U_\calI\leftarrow u})=\Gamma(X_{U_\calI\leftarrow u'})$ for all $u\in\supp(U_\calI)$, $u'\in\supp(\tildeU_\calI)$. The learned representation $\Gamma$ is fed into a learned link function $g:\bbR^d\rightarrow \text{Im} P(Y=y|X)$, $\text{Im} P(\argdot)$ being the image of $P(\argdot)$, which produces the prediction of the model, i.e.,
\[
\hat{Y}|X \sim g(\Gamma(X)) \;.
\]
For training data $X^\text{(tr)}$, if
\[
Y|X^\text{(tr)} \equdist \hat{Y}|X^\text{(tr)}\sim g_{\text{true}}(\Gamma_{\text{true}}(X^\text{(tr)}))
\]
and $\Gamma_\text{true}(X) = \Gamma_\text{true}(X_{U_\calI\leftarrow\tildeU_\calI})$, then $g_\text{true}\circ\Gamma_\text{true}$ extrapolates to test data $X^\text{(te)}$ in the sense that
\[
Y|X^\text{(te)} \equdist \hat{Y}|X^\text{(te)}\sim g_{\text{true}}(\Gamma_{\text{true}}(X^\text{(te)})) \;.
\]


\subsection{Assumptions and restrictions in single-environment extrapolation}

\textcite{Mouli:2021} make a number of assumptions in the setup described in the previous section in order to simplify the extrapolation problem and to allow for a feasible learning framework. The main assumptions that distinguish the current work from previous work in the literature are the ones revolving around the groups acting on the data.
\\

Unlike previous work that assumes the availability of training data from multiple environments, the problem setting considered by \citeauthor{Mouli:2021} specifically considers data from a single environment. Without additional information that suggests how data from different environments may differ, it is difficult to learn which pieces of information are environment-specific and irrelevant to the output. \citeauthor{Mouli:2021} deal with this issue by assuming a priori knowledge of how environments may differ in the form of transformation groups. The assumed groups specify the potential ways data from different environments may differ, and it is left to the learning framework to ``unlearn'' the groups that contradict the training data.
\\

Furthermore, \citeauthor{Mouli:2021} assume that the subset $\calG_\calI$ of groups is a normal subgroup of the overgroup $\calG_{\calD\cup\calI}$. This assumption is a consequence of Theorems 1 and 2, which together state that CG-invariances are G-invariances, i.e.,
\[
\Gamma(X)=\Gamma(T_\calI\circ X)
\]
for all $T_\calI\in\calG_\calI$, but G-invariances are CG-invariances only when $\calG_\calI\unlhd\calG_{\calD\cup\calI}$. As it is easier to check G-invariances than CG-invariances in practice due to its simpler definition, the normal subgroup assumption is made in order to make learning G-invariances sufficient for the objective. The proof of Theorem~1 (CG-invariance $\Rightarrow$ G-invariance) relies on the fact that for any transformation $T_\calI\in\calG_\calI$, we can rewrite
\[
T_\calI\circ X = T_\calI\circ T_{U_\calD,U_\calI\leftarrow u}\circ X^\text{(hid)} = T_{U_\calD,U_\calI\leftarrow\tilde{u}}\circ X^\text{(hid)}
\]
where $T_{U_\calD,\argdot}\in\calG_{\calD\cup\calI}$ and $u,\tilde{u}\in U_\calI$. The result then follows from repeated applications of CG-invariance and G-invariance definitions for a representation $\Gamma$. To show that not all G-invariances are CG-invariances, the counterexample in Figure~\ref{fig:counterexample} is given. The proof of Theorem~2 (G-invariance $\Rightarrow$ CG-invariance when $\calG_\calI\unlhd\calG_{\calD\cup\calI}$) uses the fact that under the normal subgroup assumption, $T_\calD\circ T_\calI\circ T_\calD^{-1}\in\calG_\calI$ for all $T_\calD\in\calG_\calD$, $T_\calI\in\calG\calI$. This also implies that
\[
T_\calD\circ T_\calI = T_\calI'\circ T_\calD
\]
for some $T_\calI'=T_\calD\circ T_\calI\circ T_\calD^{-1}$. Therefore for any transformation $T_\calI^{(1)}\circ T_\calD^{(1)}\circ T_\calI^{(2)}\circ\ldots\in\calG_{\calD\cup\calI}$, we can show CG-invariance for a representation $\Gamma$ by using G-invariance to remove the leading $\calG_\calI$ transformation, applying the above fact to swap the order of the new leading $\calG_\calD$ and $\calG_\calI$ transformations, and repeating the procedure until only $\calG_\calD$ transformations remain. CG-invariance then follows from definition. It is worth noting that Theorems~1 and 2 do not make additional assumptions on the groups themselves, and so these results generalize to groups beyond finite linear automorphisms.
\begin{figure}[H]
\centering
\includegraphics[width=0.52\textwidth]{counterexample.png}
\caption{A counterexample that shows not all G-invariances are CG-invariances. The goal of the example problem is to determine the orientation of an upright or flat rod in an image. A representation that sums the middle row of the image is (G-)invariant to horizontal translations of the image but not invariant to 90$^o$ rotations. Applying a translation before a rotation may result in a representation different from just applying a rotation, and so the representation is not CG-invariant. Figure from \parencite{Mouli:2021}.}
\label{fig:counterexample}
\end{figure}

While a set of transformation groups are assumed a priori, it is not known which groups represent information causally related to the output. Additionally, some of the groups may actually specify transformations that contradict the training data. \citeauthor{Mouli:2021} address this problem in their proposed framework by restricting the groups to be finite linear automorphisms. Under this restriction, the Reynolds operator given by
\[
\bar{T} = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
can be computed for each group $\calG_1,\ldots,\calG_m$. The Reynolds operator is a projection operator (Lemma~1) and so it only has 1 and 0 eigenvalues. The 1-eigenspace $\calW$ of the Reynolds operator for a group $\calG$ corresponds to the subspace of linear transformations $\gamma(x;w,b)=w^Tx+b$, $b\in\bbR$, that is invariant to all transformations $T\in\calG$, i.e.,
\[
\gamma(Tx;w,b) = \gamma(x;w,b)
\]
if and only if $w\in\calW$ (Lemma~2). The invariant subspace $\calB_M$ for a set of groups can then be computed by taking the intersection of the invariant subspaces $\calW_i$ for group $\calG_i$ in the set, i.e.,
\[
\widetilde{B}_M = \bigcap_{i\in M} \calW_i \;,
\]
and removing from the intersection its projection onto the subspace $\calB_{\supsetneq M}=\bigoplus_{N\supsetneq M}\calB_N$ formed by direct sums of the invariant subspace of all overgroups (Theorem~3). Removal of the projection implies that the resulting subspace contains vectors only invariant to all the groups in the set and no more. Thus, there is a partial ordering on the invariant subspaces where a subspace corresponding to a set of groups is defined to have ``stronger'' invariance than a subspace corresponding to a smaller subset of groups. \citeauthor{Mouli:2021}'s framework is then based on finding the strongest invariant subspace that does not contradict the training data.
\\

Lemma~1 (Reynolds operator is a projection operator) is a well-known result that applies to any group with a finite measure \parencite{Mumford:1994}. However, by restricting the groups to be finite linear automorphisms, the Reynolds operator itself takes the form of a computable matrix. Therefore, its eigenspaces can be found through standard linear algebraic methods, which is beneficial for practical use. Lemma~2 (linear transformation is group-invariant if and only if its inner product vector is in the 1-eigenspace of the Reynolds operator) is a stepping stone to Theorem~3, and is mainly used to show that considerations can be restricted to the 1-eigenspace of the Reynolds operator if the goal is group invariance. Its proof follows from direct logical derivations in both directions where sufficiency is shown using the projection property of the Reynolds operator as well as the definition of eigenvectors, and necessity is shown by reasoning that any vector in the invariant subspace is an eigenvector of the Reynolds operator. Theorem~3 (invariant subspaces can be partially ordered by invariance strength) is the main result that provides rationale for the proposed learning framework: because the invariant subspaces can be partially ordered by some notion of strength, take the strongest invariant subspace that excludes only invariances to groups that lead to contradictions in the data. Its proof consists of proving each substatement in the theorem, and each subproof is generally based on direct logical reasoning. The proof first shows that there is a hierarchy of subspaces $\calB_M$ and $\calB_{\supsetneq M}$ using induction on $M$ starting with $M=\{1,\ldots,m\}$. Using properties of the subspace construction and Lemma~2, the proof then proceeds to show that the subspaces $\calB_M$ and $\calB_{\supsetneq M}$ are orthogonal for any $M$, and that the vectors in a particular subspace are invariant to only groups that the subspace was constructed from.


\iffalse
obtaining a partial ordering on the invariant subspaces in terms of their ``strength'' and encouraging learning of only the strongest invariances through a regularized objective. In order to identify the invariant subspaces and obtain a partial ordering, \citeauthor{Mouli:2021} require the transformation groups to be linear automorphisms. This restriction implies that the Reynolds operator given by
\[
\bar{T} = \frac{1}{|\calG|}\sum_{T\in\calG}T
\]
is a G-invariant projection operator (Lemma~1). The invariant subspace for a group $\calG$ is then precisely the left eigenspace corresponding to eigenvalue 1 of the Reynolds operator (Lemma~2). To construct the invariant subspace for a set of groups indexed by a set $M$ (Theorem~3), the intersection of the 1-eigenspaces for the individual groups is taken. However, the intersection may also contain vectors that are invariant to some overgroup, and so the group's projection onto the subspace of the overgroup is removed from the intersection. The subspace $\calB_M$ that remains describes the vectors that are invariant to transformations only from the set of groups that it was built from. (\todo proof) \citeauthor{Mouli:2021}'s learning framework relies on being able to identify the invariant subspace for every subset of groups. Once the subspaces have been identified, a cost can be assigned to each subspace that encourages adopting the subspace invariant to the most number of groups.
\fi


\subsection{Learning framework for single-environment extrapolation}

Under the context and assumptions described in the previous sections, the framework proposed by \textcite{Mouli:2021} aims to learn a CG-invariant representation $\Gamma$ and a link function $g$, both of which are in the form of a neural network. The representation $\Gamma$ is a neural network layer with $H\geq1$ neurons. The $h$-th neuron has the form
\[
\Gamma^{(h)}(x) = \sigma\left(x^T\left(\sum_{i=1}^B\bfB_{M_i}\bfom_{M_i,h}\right)+b_h\right)
\]
where $\sigma(\argdot)$ is a non-linear activation function, $b_h$ is a bias parameter, $\bfB_{M_i}$ is a matrix whose columns are the orthogonal basis of the invariant subspace $\calB_{M_i}$ built from the set of groups indexed by $M_i$, and $\bfom_{M_i,h}$ are the learnable parameters which correspond to the linear combination coefficients of the basis. The parameters are collected in a neuron weight matrix
\[
\bfOm =
\begin{bmatrix}
\bfom_{M_1,1} & \ldots & \bfom_{M_1,H} \\
\vdots & \ddots & \vdots \\
\bfom_{M_B,1} & \ldots & \bfom_{M_B,H}
\end{bmatrix}
\]
where $M_1,\ldots,M_B$, $B\leq \mathrm{dim}(\calX)$, are sets of indices corresponding to different subsets of groups. The optimization objective is then
\[
\hatOm,\hatb,\hatW_g = \argmin_{\bfOm,\bfb,\bfW_g}\sum_{(y^\text{(tr)},x^\text{(tr)})\in\calD^\text{(tr)}}\calL\left(y^\text{(tr)},g(\Gamma(x^\text{(tr)};\bfOm,\bfb);\bfW_g)\right)+ \lambda R(\bfOm)
\]
where $\bfW_g$ is the parameters of $g$, $\lambda>0$ is a regularization parameter, $R(\argdot)$ is the regularization penalty given by
\[
R(\bfOm) = \left|\{M_i:|M_i|>\ell, 1\leq i\leq B\}\right| + \sum_{i:|M_i|=\ell,1\leq i\leq B}\onevec\{\|\bfom_{M_i,\argdot}\|_2^2>0\}\;,
\]
and $\ell = \min\{|M_i|\cdot\onevec\{\|\bfom_{M_i,\argdot}\|_2^2>0\}: 1\leq i \leq B\}$. The integer $\ell$ describes the size of the smallest set of groups that is used by at least one neuron (i.e., non-zero weight) across all sets of groups $M_i$. The penalty $R(\argdot)$ then counts the number of sets that are larger or are equal in size to the smallest set. Therefore, this objective encourages $\Gamma$ to use the strongest subspace, i.e., one that is invariant to more groups.
\\

To use the learning framework, bases for the invariant subspaces for the power set of groups must first be computed. The procedure starts with the set of all groups and iterates through sets of decreasing size. While the procedure only needs to be run once for a particular initial set of groups, the runtime is technically exponential as the subspace needs to be computed for every set in the power set. This can be made the worst-case runtime by setting the procedure to terminate early once a subspace equal in size to the space of the input is found. \textcite{Mouli:2021} comment that it is unclear if the worst-case runtime occurs in practice.
\\

Once the bases have been computed, the neural network can be trained with the regularized objective $R(\argdot)$ using standard supervised learning algorithms. Note that $R(\argdot)$ is discrete, and so a differentiable approximation is used during training where the indicator function $\onevec\{z>0\}$ is approximated by
\[
\widetilde{\onevec}\{z>0\} = \frac{\tau z}{\tau z+1}
\]
with $\tau\geq1$ being a temperature hyperparameter.


\subsection{Analysis of technical work}

We summarize our analysis of the technical work by \textcite{Mouli:2021} from the previous sections and add a few additional comments.
\\

The main theoretical innovations include the counterfactual formulation of the single-environment extrapolation problem and the results that rationalize the proposed learning framework under the setting. The SCM setup leads to a formalization of extrapolation that can reasoned with, and the group assumption on the environment differences enables working with single-environment data. However, the group assumption is also a potential limitation of the framework where invariances for groups that are not initially specified cannot be learned.
\\

The theoretical results all use a similar proof technique based on logical derivations that follow from applying properties and definitions. In consideration of Theorems~1 and 2, the problem setting should generalize to groups beyond finite linear automorphisms. However, Lemma~2 and Theorem~3 do not directly apply to non-linear groups and so developing analogous results for other groups would be necessary in order to justify using them with the framework. Extending the setting to non-groups or even just to non-normal subgroups for which Theorem~2 does not apply would likely require a different learning framework altogether as G-invariances are no longer CG-invariances in this case.
\\

With regards to computation, generalizing the setting to non-finite groups (with a Haar measure) may pose a practical challenge where the Reynolds operator (or orbit-averaging operator in general) may not be computable exactly. The exponential-time procedure for computing invariant subspaces may perform poorly when many groups are given even with the early termination condition. \textcite{Mouli:2021} also comment that using the early termination condition with the regularized objective may lead to suboptimal learning of the invariances, and that encouraging sparsity through an additional entropy regularization term may help with the issue (which is left for future work).

\iffalse
\begin{itemize}

\item
What implications are there with infinite linear groups? Learning framework finds invariant subspace of group using properties (idempotency) of projection operator.
\begin{itemize}
\item
Reynolds operator changes to integral over normalized Haar measure of (locally compact) group? 
\item
Provably Strict Generalisation Benefit for Invariance in Kernel Methods (Elesedy 2021). Lemma 3: RKHS decomposes into space of G-invariant functions and space of functions that vanish when averaged over an orbit. Both spaces are RKHS with different kernels.
\item
Relation to normal subgroups and quotient spaces?
\end{itemize}

\item
If considering non-normal subgroups, then Theorem 2 does not hold. Have to work with CG-invariances rather than G-invariances? What exactly does this mean?

\item
Can the framework be adapted for some equivariant objective? i.e., learning equivariant groups rather than invariant groups.

\item
Framework breaks down with non-groups. Can a non-group approach work? Can non-group structures be specified/linear? Can a variant of Theorem 2 be derived?

\item
Can something be done without assuming known groups? Consider largest possible linear overgroup and automatically learn the strongest invariances?

\item
Does the chosen regularizing term make sense? Is it optimal? Does its differentiable approximation affect these properties?

\item
The proposed algorithm is exponential as it involves the power set. Can it be reduced? Appendix D: ``...the algorithm stops after finding all the basis for the space vec$(\calX)$, it is unclear if the worst-case runtime occurs in practice.''

\end{itemize}
\fi